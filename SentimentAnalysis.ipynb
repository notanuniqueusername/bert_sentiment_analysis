{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing required packages"
      ],
      "metadata": {
        "id": "SCMv-oCnEBCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing necessary libraries"
      ],
      "metadata": {
        "id": "qBkBKayrEMGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Numerical operations\n",
        "import pandas as pd # Data manipulation and analysis"
      ],
      "metadata": {
        "id": "GZ0S0LWWbmup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets load"
      ],
      "metadata": {
        "id": "EyBqEbhlHYwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading training dataset and showing first 5 rows"
      ],
      "metadata": {
        "id": "OEnRMjUybsrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = pd.read_csv('SentimentAnalysis/Train.csv')\n",
        "train_set.head()"
      ],
      "metadata": {
        "id": "tRyKuwF5btZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading test dataset and showing first 5 rows"
      ],
      "metadata": {
        "id": "YRtJh-lbcBhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = pd.read_csv('SentimentAnalysis/Test.csv')\n",
        "test_set.head()"
      ],
      "metadata": {
        "id": "25l-QjKFcEjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading validation dataset and showing first 5 rows"
      ],
      "metadata": {
        "id": "SIO4zTBcFZIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_set = pd.read_csv('SentimentAnalysis/Valid.csv')\n",
        "val_set.head()"
      ],
      "metadata": {
        "id": "3N5tdBSqcINp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing\n"
      ],
      "metadata": {
        "id": "3alz58LeHi-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting all text to lowercase"
      ],
      "metadata": {
        "id": "VSvDqt1kGXXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowered(text):\n",
        "    return text.lower()\n",
        "train_set['text'] = train_set['text'].apply(lowered)\n",
        "test_set['text'] = test_set['text'].apply(lowered)\n",
        "val_set['text'] = val_set['text'].apply(lowered)"
      ],
      "metadata": {
        "id": "Yoh8dsctqaIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing duplicates entries based on text content"
      ],
      "metadata": {
        "id": "nfFYqxdsGetC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.drop_duplicates(subset=['text'], inplace=True)\n",
        "test_set.drop_duplicates(subset=['text'], inplace=True)\n",
        "val_set.drop_duplicates(subset=['text'], inplace=True)"
      ],
      "metadata": {
        "id": "W399zag4qv0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting DataFrames to Hugging Face Dataset for compatibility optimization"
      ],
      "metadata": {
        "id": "LiF0ekXhGqDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_pandas(train_set)\n",
        "test_dataset = Dataset.from_pandas(test_set)\n",
        "val_dataset = Dataset.from_pandas(val_set)"
      ],
      "metadata": {
        "id": "2J5AgxF-rPNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Showing information about each dataset"
      ],
      "metadata": {
        "id": "F-HbE_HLHOjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset, val_dataset"
      ],
      "metadata": {
        "id": "5Bi9A4Ckrg1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "iWU50i0oHunL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading BERT tokenizer"
      ],
      "metadata": {
        "id": "v4ZhLWzcHw6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "oZb7Rjowrpmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization function"
      ],
      "metadata": {
        "id": "TPKsPubhH-_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)"
      ],
      "metadata": {
        "id": "u_9KHLzVryI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying tokenization function to all datasets"
      ],
      "metadata": {
        "id": "umqyz9CwIMyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized = train_dataset.map(preprocess_function, batched=True)\n",
        "test_tokenized = test_dataset.map(preprocess_function, batched=True)\n",
        "val_tokenized = val_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "Rw-DOkGAr68A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shuffling dataset to improve generalization"
      ],
      "metadata": {
        "id": "9MdGUtImIYiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized.shuffle(seed=42)"
      ],
      "metadata": {
        "id": "FiGcT17Ws6zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model setup"
      ],
      "metadata": {
        "id": "XR3BKhmLIh_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading pre-trained model and configuring for binary classification"
      ],
      "metadata": {
        "id": "nc4WtmBEImkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
      ],
      "metadata": {
        "id": "ov-UkLWvtX5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up training configuration"
      ],
      "metadata": {
        "id": "76vltkdpJ6sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(output_dir=\"SentimentAnalysis/test_trainer\")"
      ],
      "metadata": {
        "id": "7JkLZgpbtk6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metrics setup"
      ],
      "metadata": {
        "id": "Y8rOItmxI2Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading evaluation metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "oJoR4X-UI8Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")"
      ],
      "metadata": {
        "id": "gN8_fC9CtngJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to compute multiple evaluation metrics"
      ],
      "metadata": {
        "id": "RRrL5dVjJlqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_metric(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"],\n",
        "        \"precision\": precision.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"],\n",
        "        \"recall\": recall.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "I-cuHmsMuJ-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5zpwnFz3KbZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training parameters"
      ],
      "metadata": {
        "id": "ecrSjBFfKtCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"SentimentAnalysis/test_trainer\",eval_strategy=\"epoch\",save_steps=3000,num_train_epochs= 2)"
      ],
      "metadata": {
        "id": "vZnrVy16uV66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing trainer"
      ],
      "metadata": {
        "id": "yQx2vJybK18T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    compute_metrics=eval_metric\n",
        ")"
      ],
      "metadata": {
        "id": "5Mtx0M-9vrO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "T1U-f6DiLA6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "bF66Brhrw76y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluation on validation data"
      ],
      "metadata": {
        "id": "9I8uv_MwLHer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "f7vrEWy2AS_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model saving"
      ],
      "metadata": {
        "id": "baYCDkLvLsWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuned model"
      ],
      "metadata": {
        "id": "aSfzKbldLvc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"SentimentAnalysis/model\")"
      ],
      "metadata": {
        "id": "qxXQTTMtA6Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "o2PG0RWULz70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"SentimentAnalysis/tokenizer\")"
      ],
      "metadata": {
        "id": "7npAi4ZvA-bF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}